/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
===== MODEL SUMMARY =====
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 32, 32]             896
       BatchNorm2d-2           [-1, 32, 32, 32]              64
            Conv2d-3           [-1, 32, 32, 32]           9,248
       BatchNorm2d-4           [-1, 32, 32, 32]              64
            Conv2d-5           [-1, 32, 32, 32]           9,248
       BatchNorm2d-6           [-1, 32, 32, 32]              64
        BasicBlock-7           [-1, 32, 32, 32]               0
            Conv2d-8           [-1, 32, 32, 32]           9,248
       BatchNorm2d-9           [-1, 32, 32, 32]              64
           Conv2d-10           [-1, 32, 32, 32]           9,248
      BatchNorm2d-11           [-1, 32, 32, 32]              64
       BasicBlock-12           [-1, 32, 32, 32]               0
           Conv2d-13           [-1, 32, 32, 32]           9,248
      BatchNorm2d-14           [-1, 32, 32, 32]              64
           Conv2d-15           [-1, 32, 32, 32]           9,248
      BatchNorm2d-16           [-1, 32, 32, 32]              64
       BasicBlock-17           [-1, 32, 32, 32]               0
           Conv2d-18           [-1, 32, 32, 32]           9,248
      BatchNorm2d-19           [-1, 32, 32, 32]              64
           Conv2d-20           [-1, 32, 32, 32]           9,248
      BatchNorm2d-21           [-1, 32, 32, 32]              64
       BasicBlock-22           [-1, 32, 32, 32]               0
           Conv2d-23           [-1, 64, 16, 16]          18,496
      BatchNorm2d-24           [-1, 64, 16, 16]             128
           Conv2d-25           [-1, 64, 16, 16]          36,928
      BatchNorm2d-26           [-1, 64, 16, 16]             128
           Conv2d-27           [-1, 64, 16, 16]           2,112
      BatchNorm2d-28           [-1, 64, 16, 16]             128
       BasicBlock-29           [-1, 64, 16, 16]               0
           Conv2d-30           [-1, 64, 16, 16]          36,928
      BatchNorm2d-31           [-1, 64, 16, 16]             128
           Conv2d-32           [-1, 64, 16, 16]          36,928
      BatchNorm2d-33           [-1, 64, 16, 16]             128
       BasicBlock-34           [-1, 64, 16, 16]               0
           Conv2d-35           [-1, 64, 16, 16]          36,928
      BatchNorm2d-36           [-1, 64, 16, 16]             128
           Conv2d-37           [-1, 64, 16, 16]          36,928
      BatchNorm2d-38           [-1, 64, 16, 16]             128
       BasicBlock-39           [-1, 64, 16, 16]               0
           Conv2d-40           [-1, 64, 16, 16]          36,928
      BatchNorm2d-41           [-1, 64, 16, 16]             128
           Conv2d-42           [-1, 64, 16, 16]          36,928
      BatchNorm2d-43           [-1, 64, 16, 16]             128
       BasicBlock-44           [-1, 64, 16, 16]               0
           Conv2d-45            [-1, 128, 8, 8]          73,856
      BatchNorm2d-46            [-1, 128, 8, 8]             256
           Conv2d-47            [-1, 128, 8, 8]         147,584
      BatchNorm2d-48            [-1, 128, 8, 8]             256
           Conv2d-49            [-1, 128, 8, 8]           8,320
      BatchNorm2d-50            [-1, 128, 8, 8]             256
       BasicBlock-51            [-1, 128, 8, 8]               0
           Conv2d-52            [-1, 128, 8, 8]         147,584
      BatchNorm2d-53            [-1, 128, 8, 8]             256
           Conv2d-54            [-1, 128, 8, 8]         147,584
      BatchNorm2d-55            [-1, 128, 8, 8]             256
       BasicBlock-56            [-1, 128, 8, 8]               0
           Conv2d-57            [-1, 128, 8, 8]         147,584
      BatchNorm2d-58            [-1, 128, 8, 8]             256
           Conv2d-59            [-1, 128, 8, 8]         147,584
      BatchNorm2d-60            [-1, 128, 8, 8]             256
       BasicBlock-61            [-1, 128, 8, 8]               0
           Conv2d-62            [-1, 128, 8, 8]         147,584
      BatchNorm2d-63            [-1, 128, 8, 8]             256
           Conv2d-64            [-1, 128, 8, 8]         147,584
      BatchNorm2d-65            [-1, 128, 8, 8]             256
       BasicBlock-66            [-1, 128, 8, 8]               0
           Conv2d-67            [-1, 256, 4, 4]         295,168
      BatchNorm2d-68            [-1, 256, 4, 4]             512
           Conv2d-69            [-1, 256, 4, 4]         590,080
      BatchNorm2d-70            [-1, 256, 4, 4]             512
           Conv2d-71            [-1, 256, 4, 4]          33,024
      BatchNorm2d-72            [-1, 256, 4, 4]             512
       BasicBlock-73            [-1, 256, 4, 4]               0
           Conv2d-74            [-1, 256, 4, 4]         590,080
      BatchNorm2d-75            [-1, 256, 4, 4]             512
           Conv2d-76            [-1, 256, 4, 4]         590,080
      BatchNorm2d-77            [-1, 256, 4, 4]             512
       BasicBlock-78            [-1, 256, 4, 4]               0
           Linear-79                   [-1, 10]           2,570
================================================================
Total params: 3,576,842
Trainable params: 3,576,842
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 10.00
Params size (MB): 13.64
Estimated Total Size (MB): 23.66
----------------------------------------------------------------
None
Initializing fetching CIFAR10 dataset using torchvision
Files already downloaded and verified
Files already downloaded and verified
Total Trainable Parameters : 3576842
Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 1024
Data Augmentation : ['trivial_aug', 'horizontal_flip', 'random_crop']
Epoch : 0, Training Loss : 2.025833469238281, Testing Loss : 7.881919677734375, Training Accuracy : 0.25162, Testing Accuracy : 0.1273
Epoch : 10, Training Loss : 1.1210601196289063, Testing Loss : 1.0736956481933593, Training Accuracy : 0.60108, Testing Accuracy : 0.6479
Epoch : 20, Training Loss : 0.8169056188964844, Testing Loss : 0.626451806640625, Training Accuracy : 0.71152, Testing Accuracy : 0.7883
Epoch : 30, Training Loss : 0.6800091418457032, Testing Loss : 0.4728471527099609, Training Accuracy : 0.76088, Testing Accuracy : 0.8402
Epoch : 40, Training Loss : 0.5946534069824219, Testing Loss : 0.5418668792724609, Training Accuracy : 0.79094, Testing Accuracy : 0.8353
Epoch : 50, Training Loss : 0.5303567370605469, Testing Loss : 0.39893853454589845, Training Accuracy : 0.81294, Testing Accuracy : 0.8698
Epoch : 60, Training Loss : 0.4789350018310547, Testing Loss : 0.3815237152099609, Training Accuracy : 0.8321, Testing Accuracy : 0.8795
Epoch : 70, Training Loss : 0.4432477581787109, Testing Loss : 0.39996372985839845, Training Accuracy : 0.84442, Testing Accuracy : 0.8743
Epoch : 80, Training Loss : 0.4088229278564453, Testing Loss : 0.40772601318359375, Training Accuracy : 0.85472, Testing Accuracy : 0.882
Epoch : 90, Training Loss : 0.380639306640625, Testing Loss : 0.31436131591796873, Training Accuracy : 0.867, Testing Accuracy : 0.9021
Epoch : 100, Training Loss : 0.34753574462890624, Testing Loss : 0.3026225433349609, Training Accuracy : 0.87788, Testing Accuracy : 0.9116
Epoch : 110, Training Loss : 0.33530147399902344, Testing Loss : 0.2660559524536133, Training Accuracy : 0.88132, Testing Accuracy : 0.9207
Epoch : 120, Training Loss : 0.3137423937988281, Testing Loss : 0.3317634399414062, Training Accuracy : 0.88972, Testing Accuracy : 0.9058
Epoch : 130, Training Loss : 0.29965481994628906, Testing Loss : 0.30049739532470704, Training Accuracy : 0.8952, Testing Accuracy : 0.9154
Epoch : 140, Training Loss : 0.2838453399658203, Testing Loss : 0.25465300903320315, Training Accuracy : 0.90086, Testing Accuracy : 0.9254
Epoch : 150, Training Loss : 0.26987281219482423, Testing Loss : 0.2428004364013672, Training Accuracy : 0.90556, Testing Accuracy : 0.9323
Epoch : 160, Training Loss : 0.25753881286621094, Testing Loss : 0.24785817413330077, Training Accuracy : 0.91006, Testing Accuracy : 0.9286
Epoch : 170, Training Loss : 0.25437673065185545, Testing Loss : 0.24390558624267578, Training Accuracy : 0.91156, Testing Accuracy : 0.9323
Epoch : 180, Training Loss : 0.24744212280273437, Testing Loss : 0.23835221252441408, Training Accuracy : 0.91344, Testing Accuracy : 0.9337
Epoch : 190, Training Loss : 0.24393312896728517, Testing Loss : 0.23981162719726562, Training Accuracy : 0.91512, Testing Accuracy : 0.9363
Epoch : 200, Training Loss : 0.23845498443603516, Testing Loss : 0.23025796356201172, Training Accuracy : 0.91608, Testing Accuracy : 0.936
Maximum Testing Accuracy Achieved: 0.9378
