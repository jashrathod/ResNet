/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
===== MODEL SUMMARY =====
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 32, 32]             896
       BatchNorm2d-2           [-1, 32, 32, 32]              64
            Conv2d-3           [-1, 32, 32, 32]           9,248
       BatchNorm2d-4           [-1, 32, 32, 32]              64
            Conv2d-5           [-1, 32, 32, 32]           9,248
       BatchNorm2d-6           [-1, 32, 32, 32]              64
        BasicBlock-7           [-1, 32, 32, 32]               0
            Conv2d-8           [-1, 32, 32, 32]           9,248
       BatchNorm2d-9           [-1, 32, 32, 32]              64
           Conv2d-10           [-1, 32, 32, 32]           9,248
      BatchNorm2d-11           [-1, 32, 32, 32]              64
       BasicBlock-12           [-1, 32, 32, 32]               0
           Conv2d-13           [-1, 32, 32, 32]           9,248
      BatchNorm2d-14           [-1, 32, 32, 32]              64
           Conv2d-15           [-1, 32, 32, 32]           9,248
      BatchNorm2d-16           [-1, 32, 32, 32]              64
       BasicBlock-17           [-1, 32, 32, 32]               0
           Conv2d-18           [-1, 32, 32, 32]           9,248
      BatchNorm2d-19           [-1, 32, 32, 32]              64
           Conv2d-20           [-1, 32, 32, 32]           9,248
      BatchNorm2d-21           [-1, 32, 32, 32]              64
       BasicBlock-22           [-1, 32, 32, 32]               0
           Conv2d-23           [-1, 64, 16, 16]          18,496
      BatchNorm2d-24           [-1, 64, 16, 16]             128
           Conv2d-25           [-1, 64, 16, 16]          36,928
      BatchNorm2d-26           [-1, 64, 16, 16]             128
           Conv2d-27           [-1, 64, 16, 16]           2,112
      BatchNorm2d-28           [-1, 64, 16, 16]             128
       BasicBlock-29           [-1, 64, 16, 16]               0
           Conv2d-30           [-1, 64, 16, 16]          36,928
      BatchNorm2d-31           [-1, 64, 16, 16]             128
           Conv2d-32           [-1, 64, 16, 16]          36,928
      BatchNorm2d-33           [-1, 64, 16, 16]             128
       BasicBlock-34           [-1, 64, 16, 16]               0
           Conv2d-35           [-1, 64, 16, 16]          36,928
      BatchNorm2d-36           [-1, 64, 16, 16]             128
           Conv2d-37           [-1, 64, 16, 16]          36,928
      BatchNorm2d-38           [-1, 64, 16, 16]             128
       BasicBlock-39           [-1, 64, 16, 16]               0
           Conv2d-40           [-1, 64, 16, 16]          36,928
      BatchNorm2d-41           [-1, 64, 16, 16]             128
           Conv2d-42           [-1, 64, 16, 16]          36,928
      BatchNorm2d-43           [-1, 64, 16, 16]             128
       BasicBlock-44           [-1, 64, 16, 16]               0
           Conv2d-45            [-1, 128, 8, 8]          73,856
      BatchNorm2d-46            [-1, 128, 8, 8]             256
           Conv2d-47            [-1, 128, 8, 8]         147,584
      BatchNorm2d-48            [-1, 128, 8, 8]             256
           Conv2d-49            [-1, 128, 8, 8]           8,320
      BatchNorm2d-50            [-1, 128, 8, 8]             256
       BasicBlock-51            [-1, 128, 8, 8]               0
           Conv2d-52            [-1, 128, 8, 8]         147,584
      BatchNorm2d-53            [-1, 128, 8, 8]             256
           Conv2d-54            [-1, 128, 8, 8]         147,584
      BatchNorm2d-55            [-1, 128, 8, 8]             256
       BasicBlock-56            [-1, 128, 8, 8]               0
           Conv2d-57            [-1, 128, 8, 8]         147,584
      BatchNorm2d-58            [-1, 128, 8, 8]             256
           Conv2d-59            [-1, 128, 8, 8]         147,584
      BatchNorm2d-60            [-1, 128, 8, 8]             256
       BasicBlock-61            [-1, 128, 8, 8]               0
           Conv2d-62            [-1, 128, 8, 8]         147,584
      BatchNorm2d-63            [-1, 128, 8, 8]             256
           Conv2d-64            [-1, 128, 8, 8]         147,584
      BatchNorm2d-65            [-1, 128, 8, 8]             256
       BasicBlock-66            [-1, 128, 8, 8]               0
           Conv2d-67            [-1, 256, 4, 4]         295,168
      BatchNorm2d-68            [-1, 256, 4, 4]             512
           Conv2d-69            [-1, 256, 4, 4]         590,080
      BatchNorm2d-70            [-1, 256, 4, 4]             512
           Conv2d-71            [-1, 256, 4, 4]          33,024
      BatchNorm2d-72            [-1, 256, 4, 4]             512
       BasicBlock-73            [-1, 256, 4, 4]               0
           Conv2d-74            [-1, 256, 4, 4]         590,080
      BatchNorm2d-75            [-1, 256, 4, 4]             512
           Conv2d-76            [-1, 256, 4, 4]         590,080
      BatchNorm2d-77            [-1, 256, 4, 4]             512
       BasicBlock-78            [-1, 256, 4, 4]               0
           Linear-79                   [-1, 10]           2,570
================================================================
Total params: 3,576,842
Trainable params: 3,576,842
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 10.00
Params size (MB): 13.64
Estimated Total Size (MB): 23.66
----------------------------------------------------------------
None
Initializing fetching CIFAR10 dataset using torchvision
Files already downloaded and verified
Files already downloaded and verified
Total Trainable Parameters : 3576842
Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 512
Data Augmentation : ['auto_aug', 'horizontal_flip', 'random_crop']
Epoch : 0, Training Loss : 1.9572230590820312, Testing Loss : 1.9608667175292969, Training Accuracy : 0.27436, Testing Accuracy : 0.323
Epoch : 10, Training Loss : 0.8907285797119141, Testing Loss : 0.860331005859375, Training Accuracy : 0.68632, Testing Accuracy : 0.7017
Epoch : 20, Training Loss : 0.5977773248291015, Testing Loss : 0.5434901229858399, Training Accuracy : 0.79086, Testing Accuracy : 0.815
Epoch : 30, Training Loss : 0.47571268463134764, Testing Loss : 0.49361533203125, Training Accuracy : 0.83454, Testing Accuracy : 0.8368
Epoch : 40, Training Loss : 0.38760238006591796, Testing Loss : 0.3865193778991699, Training Accuracy : 0.8659, Testing Accuracy : 0.8746
Epoch : 50, Training Loss : 0.3307043894958496, Testing Loss : 0.3672979248046875, Training Accuracy : 0.8852, Testing Accuracy : 0.889
Epoch : 60, Training Loss : 0.2889306887817383, Testing Loss : 0.38073071517944335, Training Accuracy : 0.89912, Testing Accuracy : 0.8845
Epoch : 70, Training Loss : 0.2468631428527832, Testing Loss : 0.3802190170288086, Training Accuracy : 0.9142, Testing Accuracy : 0.8953
Epoch : 80, Training Loss : 0.2173754495239258, Testing Loss : 0.34109116287231445, Training Accuracy : 0.92374, Testing Accuracy : 0.9035
Epoch : 90, Training Loss : 0.1993601287841797, Testing Loss : 0.31842888946533204, Training Accuracy : 0.93054, Testing Accuracy : 0.9094
Epoch : 100, Training Loss : 0.17443248313903809, Testing Loss : 0.32374161071777346, Training Accuracy : 0.93876, Testing Accuracy : 0.9168
Epoch : 110, Training Loss : 0.1570451259613037, Testing Loss : 0.31759529342651366, Training Accuracy : 0.94526, Testing Accuracy : 0.9189
Epoch : 120, Training Loss : 0.14895596031188965, Testing Loss : 0.3762085105895996, Training Accuracy : 0.94874, Testing Accuracy : 0.9081
Epoch : 130, Training Loss : 0.13765611488342286, Testing Loss : 0.32869263916015623, Training Accuracy : 0.9525, Testing Accuracy : 0.9209
Epoch : 140, Training Loss : 0.1195207657623291, Testing Loss : 0.32187438583374023, Training Accuracy : 0.95784, Testing Accuracy : 0.9259
Epoch : 150, Training Loss : 0.11419825004577637, Testing Loss : 0.30486765060424803, Training Accuracy : 0.95956, Testing Accuracy : 0.9323
Epoch : 160, Training Loss : 0.10617498695373535, Testing Loss : 0.29612685775756836, Training Accuracy : 0.96398, Testing Accuracy : 0.9347
Epoch : 170, Training Loss : 0.09766128913879395, Testing Loss : 0.29284816093444827, Training Accuracy : 0.9662, Testing Accuracy : 0.9342
Epoch : 180, Training Loss : 0.09721423027038574, Testing Loss : 0.296976343536377, Training Accuracy : 0.96638, Testing Accuracy : 0.9331
Epoch : 190, Training Loss : 0.09496026824951172, Testing Loss : 0.2880470977783203, Training Accuracy : 0.96736, Testing Accuracy : 0.935
Epoch : 200, Training Loss : 0.08981066848754883, Testing Loss : 0.2990140739440918, Training Accuracy : 0.96926, Testing Accuracy : 0.9366
Maximum Testing Accuracy Achieved: 0.9384
