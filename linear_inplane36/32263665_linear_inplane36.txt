===== MODEL SUMMARY =====
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 36, 32, 32]           1,008
       BatchNorm2d-2           [-1, 36, 32, 32]              72
            Conv2d-3           [-1, 36, 32, 32]          11,700
       BatchNorm2d-4           [-1, 36, 32, 32]              72
            Conv2d-5           [-1, 36, 32, 32]          11,700
       BatchNorm2d-6           [-1, 36, 32, 32]              72
        BasicBlock-7           [-1, 36, 32, 32]               0
            Conv2d-8           [-1, 36, 32, 32]          11,700
       BatchNorm2d-9           [-1, 36, 32, 32]              72
           Conv2d-10           [-1, 36, 32, 32]          11,700
      BatchNorm2d-11           [-1, 36, 32, 32]              72
       BasicBlock-12           [-1, 36, 32, 32]               0
           Conv2d-13           [-1, 36, 32, 32]          11,700
      BatchNorm2d-14           [-1, 36, 32, 32]              72
           Conv2d-15           [-1, 36, 32, 32]          11,700
      BatchNorm2d-16           [-1, 36, 32, 32]              72
       BasicBlock-17           [-1, 36, 32, 32]               0
           Conv2d-18           [-1, 36, 32, 32]          11,700
      BatchNorm2d-19           [-1, 36, 32, 32]              72
           Conv2d-20           [-1, 36, 32, 32]          11,700
      BatchNorm2d-21           [-1, 36, 32, 32]              72
       BasicBlock-22           [-1, 36, 32, 32]               0
           Conv2d-23           [-1, 72, 16, 16]          23,400
      BatchNorm2d-24           [-1, 72, 16, 16]             144
           Conv2d-25           [-1, 72, 16, 16]          46,728
      BatchNorm2d-26           [-1, 72, 16, 16]             144
           Conv2d-27           [-1, 72, 16, 16]           2,664
      BatchNorm2d-28           [-1, 72, 16, 16]             144
       BasicBlock-29           [-1, 72, 16, 16]               0
           Conv2d-30           [-1, 72, 16, 16]          46,728
      BatchNorm2d-31           [-1, 72, 16, 16]             144
           Conv2d-32           [-1, 72, 16, 16]          46,728
      BatchNorm2d-33           [-1, 72, 16, 16]             144
       BasicBlock-34           [-1, 72, 16, 16]               0
           Conv2d-35           [-1, 72, 16, 16]          46,728
      BatchNorm2d-36           [-1, 72, 16, 16]             144
           Conv2d-37           [-1, 72, 16, 16]          46,728
      BatchNorm2d-38           [-1, 72, 16, 16]             144
       BasicBlock-39           [-1, 72, 16, 16]               0
           Conv2d-40           [-1, 72, 16, 16]          46,728
      BatchNorm2d-41           [-1, 72, 16, 16]             144
           Conv2d-42           [-1, 72, 16, 16]          46,728
      BatchNorm2d-43           [-1, 72, 16, 16]             144
       BasicBlock-44           [-1, 72, 16, 16]               0
           Conv2d-45            [-1, 144, 8, 8]          93,456
      BatchNorm2d-46            [-1, 144, 8, 8]             288
           Conv2d-47            [-1, 144, 8, 8]         186,768
      BatchNorm2d-48            [-1, 144, 8, 8]             288
           Conv2d-49            [-1, 144, 8, 8]          10,512
      BatchNorm2d-50            [-1, 144, 8, 8]             288
       BasicBlock-51            [-1, 144, 8, 8]               0
           Conv2d-52            [-1, 144, 8, 8]         186,768
      BatchNorm2d-53            [-1, 144, 8, 8]             288
           Conv2d-54            [-1, 144, 8, 8]         186,768
      BatchNorm2d-55            [-1, 144, 8, 8]             288
       BasicBlock-56            [-1, 144, 8, 8]               0
           Conv2d-57            [-1, 144, 8, 8]         186,768
      BatchNorm2d-58            [-1, 144, 8, 8]             288
           Conv2d-59            [-1, 144, 8, 8]         186,768
      BatchNorm2d-60            [-1, 144, 8, 8]             288
       BasicBlock-61            [-1, 144, 8, 8]               0
           Conv2d-62            [-1, 144, 8, 8]         186,768
      BatchNorm2d-63            [-1, 144, 8, 8]             288
           Conv2d-64            [-1, 144, 8, 8]         186,768
      BatchNorm2d-65            [-1, 144, 8, 8]             288
       BasicBlock-66            [-1, 144, 8, 8]               0
           Conv2d-67            [-1, 288, 4, 4]         373,536
      BatchNorm2d-68            [-1, 288, 4, 4]             576
           Conv2d-69            [-1, 288, 4, 4]         746,784
      BatchNorm2d-70            [-1, 288, 4, 4]             576
           Conv2d-71            [-1, 288, 4, 4]          41,760
      BatchNorm2d-72            [-1, 288, 4, 4]             576
       BasicBlock-73            [-1, 288, 4, 4]               0
           Conv2d-74            [-1, 288, 4, 4]         746,784
      BatchNorm2d-75            [-1, 288, 4, 4]             576
           Conv2d-76            [-1, 288, 4, 4]         746,784
      BatchNorm2d-77            [-1, 288, 4, 4]             576
       BasicBlock-78            [-1, 288, 4, 4]               0
           Linear-79                  [-1, 288]          83,232
           Linear-80                   [-1, 10]           2,890
================================================================
Total params: 4,608,298
Trainable params: 4,608,298
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 11.25
Params size (MB): 17.58
Estimated Total Size (MB): 28.84
----------------------------------------------------------------
None
Initializing fetching CIFAR10 dataset using torchvision
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
  0%|          | 0/170498071 [00:00<?, ?it/s]  1%|          | 1441792/170498071 [00:00<00:11, 14414909.38it/s]  2%|▏         | 2949120/170498071 [00:00<00:11, 14791623.81it/s]  3%|▎         | 4554752/170498071 [00:00<00:10, 15200085.75it/s]  4%|▎         | 6258688/170498071 [00:00<00:10, 15856882.76it/s]  5%|▍         | 7929856/170498071 [00:00<00:10, 16097689.93it/s]  6%|▌         | 9601024/170498071 [00:00<00:09, 16113501.37it/s]  7%|▋         | 11370496/170498071 [00:00<00:09, 16532216.61it/s]  8%|▊         | 13107200/170498071 [00:00<00:09, 16785889.08it/s]  9%|▊         | 14909440/170498071 [00:00<00:09, 17123306.32it/s] 10%|▉         | 16744448/170498071 [00:01<00:08, 17429616.35it/s] 11%|█         | 18644992/170498071 [00:01<00:08, 17871610.77it/s] 12%|█▏        | 20512768/170498071 [00:01<00:08, 18067763.86it/s] 13%|█▎        | 22446080/170498071 [00:01<00:08, 18328895.05it/s] 14%|█▍        | 24444928/170498071 [00:01<00:07, 18739788.86it/s] 16%|█▌        | 26443776/170498071 [00:01<00:07, 19046885.60it/s] 17%|█▋        | 28475392/170498071 [00:01<00:07, 19312046.61it/s] 18%|█▊        | 30572544/170498071 [00:01<00:07, 19685194.19it/s] 19%|█▉        | 32669696/170498071 [00:01<00:06, 19932834.73it/s] 20%|██        | 34799616/170498071 [00:01<00:06, 20290787.91it/s] 22%|██▏       | 36962304/170498071 [00:02<00:06, 20613527.70it/s] 23%|██▎       | 39157760/170498071 [00:02<00:06, 20996703.35it/s] 24%|██▍       | 41385984/170498071 [00:02<00:06, 21368350.29it/s] 26%|██▌       | 43646976/170498071 [00:02<00:05, 21729416.14it/s] 27%|██▋       | 45907968/170498071 [00:02<00:05, 21992187.48it/s] 28%|██▊       | 48267264/170498071 [00:02<00:05, 22458880.43it/s] 30%|██▉       | 50626560/170498071 [00:02<00:05, 22712148.30it/s] 31%|███       | 53018624/170498071 [00:02<00:05, 22911049.86it/s] 32%|███▏      | 55377920/170498071 [00:02<00:04, 23097156.73it/s] 34%|███▍      | 57868288/170498071 [00:02<00:04, 23578615.43it/s] 35%|███▌      | 60456960/170498071 [00:03<00:04, 24233675.80it/s] 37%|███▋      | 62947328/170498071 [00:03<00:04, 24400263.42it/s] 38%|███▊      | 65601536/170498071 [00:03<00:04, 25000657.33it/s] 40%|███▉      | 68124672/170498071 [00:03<00:04, 24905489.59it/s] 42%|████▏     | 70778880/170498071 [00:03<00:03, 25375816.91it/s] 43%|████▎     | 73367552/170498071 [00:03<00:03, 25476864.31it/s] 45%|████▍     | 76152832/170498071 [00:03<00:03, 26146866.06it/s] 46%|████▋     | 78872576/170498071 [00:03<00:03, 26367218.06it/s] 48%|████▊     | 81690624/170498071 [00:03<00:03, 26880101.18it/s] 50%|████▉     | 84443136/170498071 [00:03<00:03, 27025700.88it/s] 51%|█████     | 87326720/170498071 [00:04<00:03, 27528518.49it/s] 53%|█████▎    | 90112000/170498071 [00:04<00:02, 27583065.54it/s] 55%|█████▍    | 93028352/170498071 [00:04<00:02, 27939152.88it/s] 56%|█████▋    | 95977472/170498071 [00:04<00:02, 28391740.34it/s] 58%|█████▊    | 98992128/170498071 [00:04<00:02, 28305444.42it/s] 60%|█████▉    | 101908480/170498071 [00:04<00:02, 28539298.89it/s] 62%|██████▏   | 105021440/170498071 [00:04<00:02, 29303081.03it/s] 63%|██████▎   | 108101632/170498071 [00:04<00:02, 29649505.76it/s] 65%|██████▌   | 111149056/170498071 [00:04<00:01, 29855325.23it/s] 67%|██████▋   | 114491392/170498071 [00:04<00:01, 30796155.40it/s] 69%|██████▉   | 117669888/170498071 [00:05<00:01, 31084863.75it/s] 71%|███████   | 120979456/170498071 [00:05<00:01, 31684159.59it/s] 73%|███████▎  | 124157952/170498071 [00:05<00:01, 31625941.57it/s] 75%|███████▍  | 127533056/170498071 [00:05<00:01, 32183532.68it/s] 77%|███████▋  | 130777088/170498071 [00:05<00:01, 32211182.58it/s] 79%|███████▊  | 134152192/170498071 [00:05<00:01, 32500829.39it/s] 81%|████████  | 137461760/170498071 [00:05<00:01, 32560113.63it/s] 83%|████████▎ | 141066240/170498071 [00:05<00:00, 33378335.47it/s] 85%|████████▍ | 144408576/170498071 [00:05<00:00, 33311874.25it/s] 87%|████████▋ | 148045824/170498071 [00:05<00:00, 34127455.85it/s] 89%|████████▉ | 151486464/170498071 [00:06<00:00, 34137379.64it/s] 91%|█████████ | 155156480/170498071 [00:06<00:00, 34643725.38it/s] 93%|█████████▎| 158662656/170498071 [00:06<00:00, 34669861.87it/s] 95%|█████████▌| 162398208/170498071 [00:06<00:00, 35404117.28it/s] 97%|█████████▋| 166035456/170498071 [00:06<00:00, 35520322.98it/s]100%|█████████▉| 169836544/170498071 [00:06<00:00, 36132642.06it/s]100%|██████████| 170498071/170498071 [00:06<00:00, 25878148.51it/s]
/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Extracting ./data/cifar-10-python.tar.gz to ./data
Files already downloaded and verified
Total Trainable Parameters : 4608298
Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 512
Data Augmentation : ['trivial_aug', 'horizontal_flip', 'random_crop']
Epoch : 0, Training Loss : 2.013976901855469, Testing Loss : 3.517933135986328, Training Accuracy : 0.2485, Testing Accuracy : 0.2309
Epoch : 10, Training Loss : 1.152518256225586, Testing Loss : 0.9411725341796875, Training Accuracy : 0.5912, Testing Accuracy : 0.6683
Epoch : 20, Training Loss : 0.8398719592285157, Testing Loss : 0.6996216354370117, Training Accuracy : 0.70618, Testing Accuracy : 0.7669
Epoch : 30, Training Loss : 0.6964575003051758, Testing Loss : 0.48498338775634764, Training Accuracy : 0.7552, Testing Accuracy : 0.8405
Epoch : 40, Training Loss : 0.6067404113769531, Testing Loss : 0.4492900665283203, Training Accuracy : 0.78626, Testing Accuracy : 0.8569
Epoch : 50, Training Loss : 0.528897230834961, Testing Loss : 0.43391300506591796, Training Accuracy : 0.81492, Testing Accuracy : 0.865
Epoch : 60, Training Loss : 0.47932769317626955, Testing Loss : 0.344459822845459, Training Accuracy : 0.83166, Testing Accuracy : 0.8885
Epoch : 70, Training Loss : 0.43556147247314453, Testing Loss : 0.3427919578552246, Training Accuracy : 0.84666, Testing Accuracy : 0.8948
Epoch : 80, Training Loss : 0.39583358673095703, Testing Loss : 0.3183783760070801, Training Accuracy : 0.86194, Testing Accuracy : 0.9073
Epoch : 90, Training Loss : 0.3717720391845703, Testing Loss : 0.2916250907897949, Training Accuracy : 0.87008, Testing Accuracy : 0.9146
Epoch : 100, Training Loss : 0.34580881744384767, Testing Loss : 0.3074286643981934, Training Accuracy : 0.87748, Testing Accuracy : 0.9154
Epoch : 110, Training Loss : 0.3312768603515625, Testing Loss : 0.2835481628417969, Training Accuracy : 0.88294, Testing Accuracy : 0.9182
Epoch : 120, Training Loss : 0.31716369934082034, Testing Loss : 0.2876402320861816, Training Accuracy : 0.8889, Testing Accuracy : 0.9187
Epoch : 130, Training Loss : 0.28925710205078126, Testing Loss : 0.25396376724243164, Training Accuracy : 0.89916, Testing Accuracy : 0.9289
Epoch : 140, Training Loss : 0.2784694384765625, Testing Loss : 0.26046101455688475, Training Accuracy : 0.90142, Testing Accuracy : 0.9307
Epoch : 150, Training Loss : 0.2624005616760254, Testing Loss : 0.24788737258911134, Training Accuracy : 0.90844, Testing Accuracy : 0.9324
Epoch : 160, Training Loss : 0.25261446716308594, Testing Loss : 0.2547223320007324, Training Accuracy : 0.91276, Testing Accuracy : 0.9337
Epoch : 170, Training Loss : 0.24242321731567382, Testing Loss : 0.24240194702148438, Training Accuracy : 0.91604, Testing Accuracy : 0.9372
Epoch : 180, Training Loss : 0.23716184799194337, Testing Loss : 0.24190621185302735, Training Accuracy : 0.9173, Testing Accuracy : 0.9362
Epoch : 190, Training Loss : 0.23016426467895507, Testing Loss : 0.24230637588500978, Training Accuracy : 0.92024, Testing Accuracy : 0.9416
Epoch : 200, Training Loss : 0.22716614486694336, Testing Loss : 0.2336275947570801, Training Accuracy : 0.91964, Testing Accuracy : 0.938
Maximum Testing Accuracy Achieved: 0.942
