/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
===== MODEL SUMMARY =====
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 32, 32]             896
       BatchNorm2d-2           [-1, 32, 32, 32]              64
            Conv2d-3           [-1, 32, 32, 32]           9,248
       BatchNorm2d-4           [-1, 32, 32, 32]              64
            Conv2d-5           [-1, 32, 32, 32]           9,248
       BatchNorm2d-6           [-1, 32, 32, 32]              64
        BasicBlock-7           [-1, 32, 32, 32]               0
            Conv2d-8           [-1, 32, 32, 32]           9,248
       BatchNorm2d-9           [-1, 32, 32, 32]              64
           Conv2d-10           [-1, 32, 32, 32]           9,248
      BatchNorm2d-11           [-1, 32, 32, 32]              64
       BasicBlock-12           [-1, 32, 32, 32]               0
           Conv2d-13           [-1, 32, 32, 32]           9,248
      BatchNorm2d-14           [-1, 32, 32, 32]              64
           Conv2d-15           [-1, 32, 32, 32]           9,248
      BatchNorm2d-16           [-1, 32, 32, 32]              64
       BasicBlock-17           [-1, 32, 32, 32]               0
           Conv2d-18           [-1, 32, 32, 32]           9,248
      BatchNorm2d-19           [-1, 32, 32, 32]              64
           Conv2d-20           [-1, 32, 32, 32]           9,248
      BatchNorm2d-21           [-1, 32, 32, 32]              64
       BasicBlock-22           [-1, 32, 32, 32]               0
           Conv2d-23           [-1, 64, 16, 16]          18,496
      BatchNorm2d-24           [-1, 64, 16, 16]             128
           Conv2d-25           [-1, 64, 16, 16]          36,928
      BatchNorm2d-26           [-1, 64, 16, 16]             128
           Conv2d-27           [-1, 64, 16, 16]           2,112
      BatchNorm2d-28           [-1, 64, 16, 16]             128
       BasicBlock-29           [-1, 64, 16, 16]               0
           Conv2d-30           [-1, 64, 16, 16]          36,928
      BatchNorm2d-31           [-1, 64, 16, 16]             128
           Conv2d-32           [-1, 64, 16, 16]          36,928
      BatchNorm2d-33           [-1, 64, 16, 16]             128
       BasicBlock-34           [-1, 64, 16, 16]               0
           Conv2d-35           [-1, 64, 16, 16]          36,928
      BatchNorm2d-36           [-1, 64, 16, 16]             128
           Conv2d-37           [-1, 64, 16, 16]          36,928
      BatchNorm2d-38           [-1, 64, 16, 16]             128
       BasicBlock-39           [-1, 64, 16, 16]               0
           Conv2d-40           [-1, 64, 16, 16]          36,928
      BatchNorm2d-41           [-1, 64, 16, 16]             128
           Conv2d-42           [-1, 64, 16, 16]          36,928
      BatchNorm2d-43           [-1, 64, 16, 16]             128
       BasicBlock-44           [-1, 64, 16, 16]               0
           Conv2d-45            [-1, 128, 8, 8]          73,856
      BatchNorm2d-46            [-1, 128, 8, 8]             256
           Conv2d-47            [-1, 128, 8, 8]         147,584
      BatchNorm2d-48            [-1, 128, 8, 8]             256
           Conv2d-49            [-1, 128, 8, 8]           8,320
      BatchNorm2d-50            [-1, 128, 8, 8]             256
       BasicBlock-51            [-1, 128, 8, 8]               0
           Conv2d-52            [-1, 128, 8, 8]         147,584
      BatchNorm2d-53            [-1, 128, 8, 8]             256
           Conv2d-54            [-1, 128, 8, 8]         147,584
      BatchNorm2d-55            [-1, 128, 8, 8]             256
       BasicBlock-56            [-1, 128, 8, 8]               0
           Conv2d-57            [-1, 128, 8, 8]         147,584
      BatchNorm2d-58            [-1, 128, 8, 8]             256
           Conv2d-59            [-1, 128, 8, 8]         147,584
      BatchNorm2d-60            [-1, 128, 8, 8]             256
       BasicBlock-61            [-1, 128, 8, 8]               0
           Conv2d-62            [-1, 128, 8, 8]         147,584
      BatchNorm2d-63            [-1, 128, 8, 8]             256
           Conv2d-64            [-1, 128, 8, 8]         147,584
      BatchNorm2d-65            [-1, 128, 8, 8]             256
       BasicBlock-66            [-1, 128, 8, 8]               0
           Conv2d-67            [-1, 256, 4, 4]         295,168
      BatchNorm2d-68            [-1, 256, 4, 4]             512
           Conv2d-69            [-1, 256, 4, 4]         590,080
      BatchNorm2d-70            [-1, 256, 4, 4]             512
           Conv2d-71            [-1, 256, 4, 4]          33,024
      BatchNorm2d-72            [-1, 256, 4, 4]             512
       BasicBlock-73            [-1, 256, 4, 4]               0
           Conv2d-74            [-1, 256, 4, 4]         590,080
      BatchNorm2d-75            [-1, 256, 4, 4]             512
           Conv2d-76            [-1, 256, 4, 4]         590,080
      BatchNorm2d-77            [-1, 256, 4, 4]             512
       BasicBlock-78            [-1, 256, 4, 4]               0
           Linear-79                   [-1, 10]           2,570
================================================================
Total params: 3,576,842
Trainable params: 3,576,842
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 10.00
Params size (MB): 13.64
Estimated Total Size (MB): 23.66
----------------------------------------------------------------
None
Initializing fetching CIFAR10 dataset using torchvision
Files already downloaded and verified
Files already downloaded and verified
Total Trainable Parameters : 3576842
Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 512 | Weight Decay : 0
Data Augmentation : ['trivial_aug', 'horizontal_flip', 'random_crop']
Epoch : 0, Training Loss : 1.9727374694824218, Testing Loss : 2.1813106750488283, Training Accuracy : 0.26796, Testing Accuracy : 0.3064
Epoch : 10, Training Loss : 1.0825269116210938, Testing Loss : 0.9504633819580078, Training Accuracy : 0.61764, Testing Accuracy : 0.6826
Epoch : 20, Training Loss : 0.7921223464965821, Testing Loss : 0.5859480560302734, Training Accuracy : 0.72272, Testing Accuracy : 0.8042
Epoch : 30, Training Loss : 0.6610516073608399, Testing Loss : 0.4598143478393555, Training Accuracy : 0.76882, Testing Accuracy : 0.8443
Epoch : 40, Training Loss : 0.5600498434448242, Testing Loss : 0.40689237060546873, Training Accuracy : 0.80186, Testing Accuracy : 0.8672
Epoch : 50, Training Loss : 0.5002683923339843, Testing Loss : 0.3671265396118164, Training Accuracy : 0.8223, Testing Accuracy : 0.8809
Epoch : 60, Training Loss : 0.4589108822631836, Testing Loss : 0.3420145057678223, Training Accuracy : 0.8386, Testing Accuracy : 0.8921
Epoch : 70, Training Loss : 0.41724725067138674, Testing Loss : 0.31610013885498045, Training Accuracy : 0.85264, Testing Accuracy : 0.9003
Epoch : 80, Training Loss : 0.3943755987548828, Testing Loss : 0.312612483215332, Training Accuracy : 0.86056, Testing Accuracy : 0.9024
Epoch : 90, Training Loss : 0.3566911810302734, Testing Loss : 0.2848143455505371, Training Accuracy : 0.8732, Testing Accuracy : 0.9145
Epoch : 100, Training Loss : 0.3320376194763184, Testing Loss : 0.2781408805847168, Training Accuracy : 0.88324, Testing Accuracy : 0.9199
Epoch : 110, Training Loss : 0.32075899337768554, Testing Loss : 0.2843856872558594, Training Accuracy : 0.88734, Testing Accuracy : 0.9202
Epoch : 120, Training Loss : 0.30126626266479495, Testing Loss : 0.32230001220703125, Training Accuracy : 0.89526, Testing Accuracy : 0.9137
Epoch : 130, Training Loss : 0.2860833235168457, Testing Loss : 0.27294051513671874, Training Accuracy : 0.89944, Testing Accuracy : 0.927
Epoch : 140, Training Loss : 0.26622677505493164, Testing Loss : 0.251672444152832, Training Accuracy : 0.90738, Testing Accuracy : 0.9312
Epoch : 150, Training Loss : 0.25196840682983396, Testing Loss : 0.24646234741210937, Training Accuracy : 0.91134, Testing Accuracy : 0.9371
Epoch : 160, Training Loss : 0.24321794311523437, Testing Loss : 0.237205322265625, Training Accuracy : 0.91458, Testing Accuracy : 0.939
Epoch : 170, Training Loss : 0.24065820388793946, Testing Loss : 0.2363643798828125, Training Accuracy : 0.9158, Testing Accuracy : 0.9371
Epoch : 180, Training Loss : 0.2334685925292969, Testing Loss : 0.23732001113891601, Training Accuracy : 0.9181, Testing Accuracy : 0.9377
Epoch : 190, Training Loss : 0.22729644195556642, Testing Loss : 0.23453748550415038, Training Accuracy : 0.92022, Testing Accuracy : 0.9382
Epoch : 200, Training Loss : 0.2236046273803711, Testing Loss : 0.225936336517334, Training Accuracy : 0.9209, Testing Accuracy : 0.9402
Maximum Testing Accuracy Achieved: 0.9426
