/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
===== MODEL SUMMARY =====
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 32, 32]             896
       BatchNorm2d-2           [-1, 32, 32, 32]              64
            Conv2d-3           [-1, 32, 32, 32]           9,248
       BatchNorm2d-4           [-1, 32, 32, 32]              64
            Conv2d-5           [-1, 32, 32, 32]           9,248
       BatchNorm2d-6           [-1, 32, 32, 32]              64
        BasicBlock-7           [-1, 32, 32, 32]               0
            Conv2d-8           [-1, 32, 32, 32]           9,248
       BatchNorm2d-9           [-1, 32, 32, 32]              64
           Conv2d-10           [-1, 32, 32, 32]           9,248
      BatchNorm2d-11           [-1, 32, 32, 32]              64
       BasicBlock-12           [-1, 32, 32, 32]               0
           Conv2d-13           [-1, 32, 32, 32]           9,248
      BatchNorm2d-14           [-1, 32, 32, 32]              64
           Conv2d-15           [-1, 32, 32, 32]           9,248
      BatchNorm2d-16           [-1, 32, 32, 32]              64
       BasicBlock-17           [-1, 32, 32, 32]               0
           Conv2d-18           [-1, 32, 32, 32]           9,248
      BatchNorm2d-19           [-1, 32, 32, 32]              64
           Conv2d-20           [-1, 32, 32, 32]           9,248
      BatchNorm2d-21           [-1, 32, 32, 32]              64
       BasicBlock-22           [-1, 32, 32, 32]               0
           Conv2d-23           [-1, 64, 16, 16]          18,496
      BatchNorm2d-24           [-1, 64, 16, 16]             128
           Conv2d-25           [-1, 64, 16, 16]          36,928
      BatchNorm2d-26           [-1, 64, 16, 16]             128
           Conv2d-27           [-1, 64, 16, 16]           2,112
      BatchNorm2d-28           [-1, 64, 16, 16]             128
       BasicBlock-29           [-1, 64, 16, 16]               0
           Conv2d-30           [-1, 64, 16, 16]          36,928
      BatchNorm2d-31           [-1, 64, 16, 16]             128
           Conv2d-32           [-1, 64, 16, 16]          36,928
      BatchNorm2d-33           [-1, 64, 16, 16]             128
       BasicBlock-34           [-1, 64, 16, 16]               0
           Conv2d-35           [-1, 64, 16, 16]          36,928
      BatchNorm2d-36           [-1, 64, 16, 16]             128
           Conv2d-37           [-1, 64, 16, 16]          36,928
      BatchNorm2d-38           [-1, 64, 16, 16]             128
       BasicBlock-39           [-1, 64, 16, 16]               0
           Conv2d-40           [-1, 64, 16, 16]          36,928
      BatchNorm2d-41           [-1, 64, 16, 16]             128
           Conv2d-42           [-1, 64, 16, 16]          36,928
      BatchNorm2d-43           [-1, 64, 16, 16]             128
       BasicBlock-44           [-1, 64, 16, 16]               0
           Conv2d-45            [-1, 128, 8, 8]          73,856
      BatchNorm2d-46            [-1, 128, 8, 8]             256
           Conv2d-47            [-1, 128, 8, 8]         147,584
      BatchNorm2d-48            [-1, 128, 8, 8]             256
           Conv2d-49            [-1, 128, 8, 8]           8,320
      BatchNorm2d-50            [-1, 128, 8, 8]             256
       BasicBlock-51            [-1, 128, 8, 8]               0
           Conv2d-52            [-1, 128, 8, 8]         147,584
      BatchNorm2d-53            [-1, 128, 8, 8]             256
           Conv2d-54            [-1, 128, 8, 8]         147,584
      BatchNorm2d-55            [-1, 128, 8, 8]             256
       BasicBlock-56            [-1, 128, 8, 8]               0
           Conv2d-57            [-1, 128, 8, 8]         147,584
      BatchNorm2d-58            [-1, 128, 8, 8]             256
           Conv2d-59            [-1, 128, 8, 8]         147,584
      BatchNorm2d-60            [-1, 128, 8, 8]             256
       BasicBlock-61            [-1, 128, 8, 8]               0
           Conv2d-62            [-1, 128, 8, 8]         147,584
      BatchNorm2d-63            [-1, 128, 8, 8]             256
           Conv2d-64            [-1, 128, 8, 8]         147,584
      BatchNorm2d-65            [-1, 128, 8, 8]             256
       BasicBlock-66            [-1, 128, 8, 8]               0
           Conv2d-67            [-1, 256, 4, 4]         295,168
      BatchNorm2d-68            [-1, 256, 4, 4]             512
           Conv2d-69            [-1, 256, 4, 4]         590,080
      BatchNorm2d-70            [-1, 256, 4, 4]             512
           Conv2d-71            [-1, 256, 4, 4]          33,024
      BatchNorm2d-72            [-1, 256, 4, 4]             512
       BasicBlock-73            [-1, 256, 4, 4]               0
           Conv2d-74            [-1, 256, 4, 4]         590,080
      BatchNorm2d-75            [-1, 256, 4, 4]             512
           Conv2d-76            [-1, 256, 4, 4]         590,080
      BatchNorm2d-77            [-1, 256, 4, 4]             512
       BasicBlock-78            [-1, 256, 4, 4]               0
           Linear-79                   [-1, 10]           2,570
================================================================
Total params: 3,576,842
Trainable params: 3,576,842
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 10.00
Params size (MB): 13.64
Estimated Total Size (MB): 23.66
----------------------------------------------------------------
None
Initializing fetching CIFAR10 dataset using torchvision
Files already downloaded and verified
Files already downloaded and verified
Total Trainable Parameters : 3576842
Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 512
Data Augmentation : ['trivial_aug', 'horizontal_flip', 'random_crop']
Epoch : 0, Training Loss : 1.9901696435546874, Testing Loss : 2.769219091796875, Training Accuracy : 0.26518, Testing Accuracy : 0.2724
Epoch : 10, Training Loss : 1.1118037561035157, Testing Loss : 0.8703910446166992, Training Accuracy : 0.60602, Testing Accuracy : 0.6931
Epoch : 20, Training Loss : 0.8094199490356445, Testing Loss : 0.5738830917358398, Training Accuracy : 0.71666, Testing Accuracy : 0.8049
Epoch : 30, Training Loss : 0.6777064266967774, Testing Loss : 0.49557508087158203, Training Accuracy : 0.75942, Testing Accuracy : 0.8319
Epoch : 40, Training Loss : 0.5649311486816406, Testing Loss : 0.43079615020751955, Training Accuracy : 0.80206, Testing Accuracy : 0.8587
Epoch : 50, Training Loss : 0.520218186340332, Testing Loss : 0.3806113739013672, Training Accuracy : 0.81664, Testing Accuracy : 0.8774
Epoch : 60, Training Loss : 0.47721685363769534, Testing Loss : 0.3437335395812988, Training Accuracy : 0.83268, Testing Accuracy : 0.8923
Epoch : 70, Training Loss : 0.4280904705810547, Testing Loss : 0.34549842376708984, Training Accuracy : 0.84904, Testing Accuracy : 0.8905
Epoch : 80, Training Loss : 0.4002242135620117, Testing Loss : 0.2952796806335449, Training Accuracy : 0.85834, Testing Accuracy : 0.9068
Epoch : 90, Training Loss : 0.36518240005493163, Testing Loss : 0.32159394226074217, Training Accuracy : 0.86968, Testing Accuracy : 0.9055
Epoch : 100, Training Loss : 0.3390485734558105, Testing Loss : 0.2851670074462891, Training Accuracy : 0.88272, Testing Accuracy : 0.9155
Epoch : 110, Training Loss : 0.32376418716430666, Testing Loss : 0.26547688751220705, Training Accuracy : 0.88504, Testing Accuracy : 0.9197
Epoch : 120, Training Loss : 0.3021149217224121, Testing Loss : 0.2748316749572754, Training Accuracy : 0.89604, Testing Accuracy : 0.9203
Epoch : 130, Training Loss : 0.29072638610839846, Testing Loss : 0.2594019889831543, Training Accuracy : 0.89898, Testing Accuracy : 0.9286
Epoch : 140, Training Loss : 0.27460913208007814, Testing Loss : 0.2297925937652588, Training Accuracy : 0.90318, Testing Accuracy : 0.936
Epoch : 150, Training Loss : 0.25506467193603516, Testing Loss : 0.24123475875854491, Training Accuracy : 0.91012, Testing Accuracy : 0.9366
Epoch : 160, Training Loss : 0.24802217727661133, Testing Loss : 0.24673473663330078, Training Accuracy : 0.91342, Testing Accuracy : 0.9354
Epoch : 170, Training Loss : 0.2434607846069336, Testing Loss : 0.23250119247436524, Training Accuracy : 0.91524, Testing Accuracy : 0.9378
Epoch : 180, Training Loss : 0.23728071441650392, Testing Loss : 0.22839380950927735, Training Accuracy : 0.91714, Testing Accuracy : 0.9401
Epoch : 190, Training Loss : 0.23275970001220703, Testing Loss : 0.2326712890625, Training Accuracy : 0.91848, Testing Accuracy : 0.942
Epoch : 200, Training Loss : 0.23035620681762695, Testing Loss : 0.22705747146606445, Training Accuracy : 0.91846, Testing Accuracy : 0.9404
Maximum Testing Accuracy Achieved: 0.9441
