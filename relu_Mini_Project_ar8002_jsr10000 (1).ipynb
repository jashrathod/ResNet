{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ar8002/.local/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in /home/ar8002/.local/lib/python3.8/site-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in /home/ar8002/.local/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: networkx in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: filelock in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: jinja2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: sympy in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ar8002/.local/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: requests in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: numpy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: wheel in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (49.2.1)\n",
      "Requirement already satisfied: cmake in /home/ar8002/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/ar8002/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (16.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ar8002/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (3.0.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchsummary in /home/ar8002/.local/lib/python3.8/site-packages (1.5.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ar8002/.local/lib/python3.8/site-packages (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip3 install torch torchvision torchaudio\n",
    "pip3 install torchsummary\n",
    "pip3 install tqdm\n",
    "pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Zwq45lv0w_ok"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "torch.manual_seed(17)\n",
    "\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f7LafQAd_GOU"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, kernel, skip_kernel, stride=1, bias=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=kernel[0], stride=stride, padding=kernel[1], bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel[0],\n",
    "                               stride=1, padding=kernel[1], bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=skip_kernel[0], padding=skip_kernel[1], stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_planes, num_layers, num_blocks, kernel, skip_kernel, num_classes=10, bias=True):\n",
    "        if not isinstance(num_blocks, list):\n",
    "            raise Exception(\"num_blocks parameter should be a list of integer values\")\n",
    "        if num_layers != len(num_blocks):\n",
    "            raise Exception(\"Residual layers should be equal to the length of num_blocks list\")\n",
    "        super(ResNet, self).__init__()\n",
    "        self.kernel = kernel\n",
    "        self.skip_kernel = skip_kernel\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=kernel[0],\n",
    "                               stride=1, padding=kernel[1], bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = self._make_layer(block, self.in_planes, num_blocks[0], stride=1, bias=bias)\n",
    "        for i in range(2, num_layers+1):\n",
    "            setattr(self, \"layer\"+str(i), self._make_layer(block, 2*self.in_planes, num_blocks[i-1], stride=2, bias=bias))\n",
    "        finalshape = list(getattr(self, \"layer\"+str(num_layers))[-1].modules())[-2].num_features\n",
    "        self.multiplier = 4 if num_layers == 2 else (2 if num_layers == 3 else 1)\n",
    "        self.linear = nn.Linear(finalshape, num_classes)\n",
    "        self.path = \"./project1_model.pt\"\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, bias=True):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        custom_layers = []\n",
    "        for stride in strides:\n",
    "            custom_layers.append(block(self.in_planes, planes,self.kernel,self.skip_kernel, stride, bias))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*custom_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        for i in range(1, self.num_layers+1):\n",
    "            out = eval(\"self.layer\" + str(i) + \"(out)\")\n",
    "        out = F.avg_pool2d(out, 4*self.multiplier)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    def saveToDisk(self):\n",
    "        torch.save(self.state_dict(), self.path)\n",
    "\n",
    "    def loadFromDisk(self):\n",
    "        self.load_state_dict(torch.load(self.path))\n",
    "\n",
    "def project1_model():\n",
    "    return ResNet(BasicBlock, 32, 4, [4, 4, 4, 2],kernel=(3,1),skip_kernel=(1,0), num_classes=10, bias=True)\n",
    "\n",
    "    model = ResNet(BasicBlock, 32, 4, [4, 4, 4, 2],kernel=(3,1),skip_kernel=(1,0), num_classes=10, bias=True)\n",
    "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(trainable_parameters)\n",
    "    x = torch.rand(1, 3, 32, 32)\n",
    "    model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1ASDQ6sOWjWY"
   },
   "outputs": [],
   "source": [
    "class DatasetFetcher:\n",
    "    def __init__(self, dataset=\"CIFAR10\", batch_size=64):\n",
    "        print(\"Initializing fetching %s dataset using torchvision\"%(dataset))\n",
    "        self.datasetObject = torchvision.datasets.__dict__.get(dataset, None)\n",
    "        if self.datasetObject == None:\n",
    "            raise Exception(\"Dataset %s not available in torchvision.\"%(dataset))\n",
    "        self.batch_size = batch_size\n",
    "        self.train_transformers = []\n",
    "        self.test_transformers = []\n",
    "        self.workersAvailable = min(multiprocessing.cpu_count(), 14)\n",
    "\n",
    "    def addHorizontalFlipping(self):\n",
    "        self.train_transformers.append(torchvision.transforms.RandomHorizontalFlip())\n",
    "\n",
    "    def addVerticalFlipping(self):\n",
    "        self.train_transformers.append(torchvision.transforms.RandomVerticalFlip())\n",
    "        \n",
    "    def addRandomCrop(self, size=32, padding=3):\n",
    "        self.train_transformers.append(torchvision.transforms.RandomCrop(size=size, padding=padding))\n",
    "        \n",
    "    def addHistogramEqualization(self):\n",
    "        self.train_transformers.append(torchvision.transforms.functional.equalize)\n",
    "        self.test_transformers.append(torchvision.transforms.functional.equalize)\n",
    "\n",
    "    def __addToTensor(self):\n",
    "        self.train_transformers.append(torchvision.transforms.ToTensor())\n",
    "        self.test_transformers.append(torchvision.transforms.ToTensor())\n",
    "        \n",
    "    def __loadTrainNormalizers(self):\n",
    "        params = np.load(\"./trainNormalizedParameters.npz\")\n",
    "        return params['mean'], params['std']\n",
    "\n",
    "    def addNormalizer(self):\n",
    "        self.__addToTensor()\n",
    "        trainingDataset = self.datasetObject(root=\"./data\", train=True, download=True)\n",
    "        trainData = trainingDataset.data/255.0\n",
    "        mean = trainData.mean(axis=(0, 1, 2))\n",
    "        std = trainData.std(axis=(0, 1, 2))\n",
    "        np.savez(\"./trainNormalizedParameters\", mean=mean, std=std)\n",
    "        self.train_transformers.append(torchvision.transforms.Normalize(mean=mean, std=std))\n",
    "        self.test_transformers.append(torchvision.transforms.Normalize(mean=mean, std=std))\n",
    "        \n",
    "    def addAutoAugmentation(self):\n",
    "        self.train_transformers.append(torchvision.transforms.AutoAugment(torchvision.transforms.AutoAugmentPolicy.CIFAR10))\n",
    "    \n",
    "    def addTrivialAugmentation(self):\n",
    "        self.train_transformers.append(torchvision.transforms.TrivialAugmentWide())\n",
    "        self.__addToTensor()\n",
    "\n",
    "    def getLoaders(self):\n",
    "        if len(self.train_transformers) == 0:\n",
    "            self.__addToTensor()\n",
    "        trainingDataset = self.datasetObject(root=\"./data\", train=True, download=True, transform=torchvision.transforms.Compose(self.train_transformers))\n",
    "        testingDataset = self.datasetObject(root=\"./data\", train=False, download=True, transform=torchvision.transforms.Compose(self.test_transformers))\n",
    "        trainLoader = DataLoader(trainingDataset, batch_size=self.batch_size, shuffle=True, num_workers=self.workersAvailable)\n",
    "        testLoader = DataLoader(testingDataset, batch_size=self.batch_size, shuffle=False, num_workers=self.workersAvailable)\n",
    "        return trainLoader, testLoader\n",
    "    \n",
    "    def getTestLoader(self):\n",
    "        mean, std = self.__loadTrainNormalizers()\n",
    "        self.test_transformers.append(torchvision.transforms.ToTensor())\n",
    "        self.test_transformers.append(torchvision.transforms.Normalize(mean=mean, std=std))\n",
    "        testingDataset = self.datasetObject(root=\"./data\", train=False, download=True, transform=torchvision.transforms.Compose(self.test_transformers))\n",
    "        testLoader = DataLoader(testingDataset, batch_size=self.batch_size, shuffle=False, num_workers=self.workersAvailable)\n",
    "        return testLoader\n",
    "\n",
    "# df = DatasetFetcher(dataset=\"CIFAR10\", batch_size=128)\n",
    "# # df.addHorizontalFlipping()\n",
    "# # df.addRandomCrop(size=32, padding=3)\n",
    "# # df.addHistogramEqualization()\n",
    "# # df.addNormalizer()\n",
    "# df.addTrivialAugmentation()\n",
    "# trainLoader, testLoader = df.getLoaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUgErRLaXzt1",
    "outputId": "32edbdf6-fe76-4aad-fcb7-fd526f1e6bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "            Conv2d-3           [-1, 32, 32, 32]           9,248\n",
      "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
      "            Conv2d-5           [-1, 32, 32, 32]           9,248\n",
      "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
      "        BasicBlock-7           [-1, 32, 32, 32]               0\n",
      "            Conv2d-8           [-1, 32, 32, 32]           9,248\n",
      "       BatchNorm2d-9           [-1, 32, 32, 32]              64\n",
      "           Conv2d-10           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
      "       BasicBlock-12           [-1, 32, 32, 32]               0\n",
      "           Conv2d-13           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-14           [-1, 32, 32, 32]              64\n",
      "           Conv2d-15           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-16           [-1, 32, 32, 32]              64\n",
      "       BasicBlock-17           [-1, 32, 32, 32]               0\n",
      "           Conv2d-18           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-19           [-1, 32, 32, 32]              64\n",
      "           Conv2d-20           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 32, 32]              64\n",
      "       BasicBlock-22           [-1, 32, 32, 32]               0\n",
      "           Conv2d-23           [-1, 64, 16, 16]          18,496\n",
      "      BatchNorm2d-24           [-1, 64, 16, 16]             128\n",
      "           Conv2d-25           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-26           [-1, 64, 16, 16]             128\n",
      "           Conv2d-27           [-1, 64, 16, 16]           2,112\n",
      "      BatchNorm2d-28           [-1, 64, 16, 16]             128\n",
      "       BasicBlock-29           [-1, 64, 16, 16]               0\n",
      "           Conv2d-30           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-31           [-1, 64, 16, 16]             128\n",
      "           Conv2d-32           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-33           [-1, 64, 16, 16]             128\n",
      "       BasicBlock-34           [-1, 64, 16, 16]               0\n",
      "           Conv2d-35           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-36           [-1, 64, 16, 16]             128\n",
      "           Conv2d-37           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-38           [-1, 64, 16, 16]             128\n",
      "       BasicBlock-39           [-1, 64, 16, 16]               0\n",
      "           Conv2d-40           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
      "           Conv2d-42           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-43           [-1, 64, 16, 16]             128\n",
      "       BasicBlock-44           [-1, 64, 16, 16]               0\n",
      "           Conv2d-45            [-1, 128, 8, 8]          73,856\n",
      "      BatchNorm2d-46            [-1, 128, 8, 8]             256\n",
      "           Conv2d-47            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
      "           Conv2d-49            [-1, 128, 8, 8]           8,320\n",
      "      BatchNorm2d-50            [-1, 128, 8, 8]             256\n",
      "       BasicBlock-51            [-1, 128, 8, 8]               0\n",
      "           Conv2d-52            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-53            [-1, 128, 8, 8]             256\n",
      "           Conv2d-54            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-55            [-1, 128, 8, 8]             256\n",
      "       BasicBlock-56            [-1, 128, 8, 8]               0\n",
      "           Conv2d-57            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-58            [-1, 128, 8, 8]             256\n",
      "           Conv2d-59            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-60            [-1, 128, 8, 8]             256\n",
      "       BasicBlock-61            [-1, 128, 8, 8]               0\n",
      "           Conv2d-62            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-63            [-1, 128, 8, 8]             256\n",
      "           Conv2d-64            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-65            [-1, 128, 8, 8]             256\n",
      "       BasicBlock-66            [-1, 128, 8, 8]               0\n",
      "           Conv2d-67            [-1, 256, 4, 4]         295,168\n",
      "      BatchNorm2d-68            [-1, 256, 4, 4]             512\n",
      "           Conv2d-69            [-1, 256, 4, 4]         590,080\n",
      "      BatchNorm2d-70            [-1, 256, 4, 4]             512\n",
      "           Conv2d-71            [-1, 256, 4, 4]          33,024\n",
      "      BatchNorm2d-72            [-1, 256, 4, 4]             512\n",
      "       BasicBlock-73            [-1, 256, 4, 4]               0\n",
      "           Conv2d-74            [-1, 256, 4, 4]         590,080\n",
      "      BatchNorm2d-75            [-1, 256, 4, 4]             512\n",
      "           Conv2d-76            [-1, 256, 4, 4]         590,080\n",
      "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
      "       BasicBlock-78            [-1, 256, 4, 4]               0\n",
      "           Linear-79                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 3,576,842\n",
      "Trainable params: 3,576,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 10.00\n",
      "Params size (MB): 13.64\n",
      "Estimated Total Size (MB): 23.66\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(BasicBlock, 32, 4, [4, 4, 4, 2],kernel=(3,1),skip_kernel=(1,0), num_classes=10, bias=True).to(device)\n",
    "print(summary(model, input_size=(3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mq6Ph5PUZtwL"
   },
   "outputs": [],
   "source": [
    "# EPOCHS=20\n",
    "# globalBestAccuracy = 0.0\n",
    "# trainingLoss = []\n",
    "# testingLoss = []\n",
    "# trainingAccuracy = []\n",
    "# testingAccuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgZDPOcbZ2JN"
   },
   "outputs": [],
   "source": [
    "# # Defining Loss Function, Learning Rate, Weight Decay, Optimizer) \n",
    "# lossFunction = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "# learningRate = 0.1\n",
    "# weightDecay = 0.0001\n",
    "# #optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "# #optimizer = torch.optim.Adagrad(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=learningRate/10.0)\n",
    "# print(model.eval())\n",
    "# trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"Total Trainable Parameters : %s\"%(trainable_parameters))\n",
    "# if trainable_parameters > 5*(10**6):\n",
    "#     raise Exception(\"Model not under budget!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vsg6Hi24aKJk"
   },
   "outputs": [],
   "source": [
    "# for i in tqdm(range(EPOCHS)):\n",
    "#     for phase in ['train', 'test']:\n",
    "#         if phase == \"train\":\n",
    "#             loader = trainLoader\n",
    "#             model.train()\n",
    "#             optimizer.zero_grad()\n",
    "#         else:\n",
    "#             loader = testLoader\n",
    "#             model.eval()\n",
    "#         runningLoss = 0.0\n",
    "#         runningCorrects = 0\n",
    "#         for images, labels in loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             output = model(images)\n",
    "#             loss = lossFunction(output, labels)\n",
    "#             predicted_labels = torch.argmax(output, dim=1)\n",
    "#             #runningLoss += loss.item()*images.size(0)\n",
    "#             runningLoss += loss.item()\n",
    "#             runningCorrects += torch.sum(predicted_labels == labels).float().item()\n",
    "#             if phase == \"train\":\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#         epochLoss = runningLoss/len(loader.dataset)\n",
    "#         epochAccuracy = runningCorrects/len(loader.dataset)\n",
    "#         if phase == \"train\":\n",
    "#             scheduler.step()\n",
    "#             trainingLoss.append(epochLoss)\n",
    "#             trainingAccuracy.append(epochAccuracy)\n",
    "#         else:\n",
    "#             testingLoss.append(epochLoss)\n",
    "#             testingAccuracy.append(epochAccuracy)\n",
    "#             if epochAccuracy > globalBestAccuracy:\n",
    "#                 globalBestAccuracy = epochAccuracy\n",
    "#                 model.saveToDisk()\n",
    "#     print(\"Training Loss : %s, Testing Loss : %s, Training Accuracy : %s, Testing Accuracy : %s\"\\\n",
    "#           %(trainingLoss[-1], testingLoss[-1], trainingAccuracy[-1], testingAccuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KooUOGbgeGE4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9hWt_MXeGBt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zl2JCGrZeF69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Baa1AhlseF3u"
   },
   "outputs": [],
   "source": [
    "optimizers_dict = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adagrad\": torch.optim.Adagrad,\n",
    "    \"adadelta\": torch.optim.Adadelta,\n",
    "    \"sgd\": torch.optim.SGD\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ymQPEjCShOLU"
   },
   "outputs": [],
   "source": [
    "def main(model, data_augmentation=['trivial_aug'], epochs=100, optim=\"adadelta\", batch_size=512, print_every=10):\n",
    "    df = DatasetFetcher(dataset=\"CIFAR10\", batch_size=batch_size)\n",
    "\n",
    "    for aug in data_augmentation:\n",
    "        if aug == 'trivial_aug':\n",
    "            df.addTrivialAugmentation()\n",
    "        elif aug == 'horizontal_flip':\n",
    "            df.addHorizontalFlipping()\n",
    "        elif aug == 'random_crop':\n",
    "            df.addRandomCrop(size=32, padding=3)\n",
    "        elif aug == 'histogram_equalization':\n",
    "            df.addHistogramEqualization()\n",
    "        elif aug == 'normalizer':\n",
    "            df.addNormalizer()\n",
    "        elif aug == 'vertical_flip':\n",
    "            df.addVerticalFlipping()\n",
    "        elif aug == 'auto_aug':\n",
    "            df.addAutoAugmentation()\n",
    "    \n",
    "    trainLoader, testLoader = df.getLoaders()\n",
    "\n",
    "    EPOCHS=epochs\n",
    "    globalBestAccuracy = 0.0\n",
    "    trainingLoss = []\n",
    "    testingLoss = []\n",
    "    trainingAccuracy = []\n",
    "    testingAccuracy = []\n",
    "\n",
    "    # Defining Loss Function, Learning Rate, Weight Decay, Optimizer) \n",
    "    lossFunction = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    learningRate = 0.1\n",
    "    weightDecay = 0.0001\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "    # optimizer = torch.optim.Adagrad(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "\n",
    "    # optimizers_dict = {\n",
    "    #     \"adam\": torch.optim.Adam,\n",
    "    #     \"adagrad\": torch.optim.Adagrad,\n",
    "    #     \"adadelta\": torch.optim.Adadelta,\n",
    "    #     \"sgd\": torch.optim.SGD\n",
    "    # }\n",
    "\n",
    "    optimizer_fn = optimizers_dict[optim]\n",
    "    optimizer = optimizer_fn(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=learningRate/10.0)\n",
    "    # print(model.eval())\n",
    "\n",
    "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Total Trainable Parameters : %s\"%(trainable_parameters))\n",
    "    \n",
    "    if trainable_parameters > 5*(10**6):\n",
    "        raise Exception(\"Model not under budget!\")\n",
    "\n",
    "    print(f\"Total Epochs : {EPOCHS} | Optimizer : {optim} | Learning Rate : {learningRate} | Batch Size : {batch_size}\")\n",
    "    print(f\"Data Augmentation : {data_augmentation}\")\n",
    "\n",
    "    # for i in tqdm(range(EPOCHS)):\n",
    "    for i in range(EPOCHS+1):\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == \"train\":\n",
    "                loader = trainLoader\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                loader = testLoader\n",
    "                model.eval()\n",
    "            runningLoss = 0.0\n",
    "            runningCorrects = 0\n",
    "            for images, labels in loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model(images)\n",
    "                loss = lossFunction(output, labels)\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                #runningLoss += loss.item()*images.size(0)\n",
    "                runningLoss += loss.item()\n",
    "                runningCorrects += torch.sum(predicted_labels == labels).float().item()\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            epochLoss = runningLoss/len(loader.dataset)\n",
    "            epochAccuracy = runningCorrects/len(loader.dataset)\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "                trainingLoss.append(epochLoss)\n",
    "                trainingAccuracy.append(epochAccuracy)\n",
    "            else:\n",
    "                testingLoss.append(epochLoss)\n",
    "                testingAccuracy.append(epochAccuracy)\n",
    "                if epochAccuracy > globalBestAccuracy:\n",
    "                    globalBestAccuracy = epochAccuracy\n",
    "                    model.saveToDisk()\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(\"Epoch : %s, Training Loss : %s, Testing Loss : %s, Training Accuracy : %s, Testing Accuracy : %s\"\\\n",
    "              %(i, trainingLoss[-1], testingLoss[-1], trainingAccuracy[-1], testingAccuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCx9IJrnhcq6",
    "outputId": "f88ed611-5143-46d9-ead8-fcbb11fcde7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing fetching CIFAR10 dataset using torchvision\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:09<00:00, 17628304.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Total Trainable Parameters : 3576842\n",
      "Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 512\n",
      "Data Augmentation : ['trivial_aug']\n",
      "Epoch : 0, Training Loss : 1.9580760949707032, Testing Loss : 1.9975725219726563, Training Accuracy : 0.28, Testing Accuracy : 0.2999\n",
      "Epoch : 10, Training Loss : 0.9269402026367187, Testing Loss : 0.8281432662963867, Training Accuracy : 0.67318, Testing Accuracy : 0.7183\n",
      "Epoch : 20, Training Loss : 0.6449001486206055, Testing Loss : 0.5719282989501953, Training Accuracy : 0.77022, Testing Accuracy : 0.8119\n",
      "Epoch : 30, Training Loss : 0.4877888565063477, Testing Loss : 0.6073644805908203, Training Accuracy : 0.83042, Testing Accuracy : 0.8254\n",
      "Epoch : 40, Training Loss : 0.42329267547607424, Testing Loss : 0.590873991394043, Training Accuracy : 0.85334, Testing Accuracy : 0.839\n",
      "Epoch : 50, Training Loss : 0.38254834869384763, Testing Loss : 0.5321096740722656, Training Accuracy : 0.86558, Testing Accuracy : 0.8546\n",
      "Epoch : 60, Training Loss : 0.34476442138671876, Testing Loss : 0.5245576766967773, Training Accuracy : 0.88012, Testing Accuracy : 0.8642\n",
      "Epoch : 70, Training Loss : 0.31829029235839845, Testing Loss : 0.5271943572998047, Training Accuracy : 0.88918, Testing Accuracy : 0.8649\n",
      "Epoch : 80, Training Loss : 0.30199774047851563, Testing Loss : 0.5015697082519531, Training Accuracy : 0.89504, Testing Accuracy : 0.8734\n",
      "Epoch : 90, Training Loss : 0.27591418243408206, Testing Loss : 0.506742497253418, Training Accuracy : 0.90374, Testing Accuracy : 0.8813\n",
      "Epoch : 100, Training Loss : 0.26799593627929685, Testing Loss : 0.45755856475830076, Training Accuracy : 0.90738, Testing Accuracy : 0.8882\n",
      "Epoch : 110, Training Loss : 0.2506558689880371, Testing Loss : 0.43287109222412107, Training Accuracy : 0.91208, Testing Accuracy : 0.8936\n",
      "Epoch : 120, Training Loss : 0.2355617854309082, Testing Loss : 0.45417028503417967, Training Accuracy : 0.9173, Testing Accuracy : 0.8909\n",
      "Epoch : 130, Training Loss : 0.22900222564697265, Testing Loss : 0.42548516998291014, Training Accuracy : 0.91948, Testing Accuracy : 0.8972\n",
      "Epoch : 140, Training Loss : 0.2136557229614258, Testing Loss : 0.3973277038574219, Training Accuracy : 0.92478, Testing Accuracy : 0.9028\n",
      "Epoch : 150, Training Loss : 0.21097809844970702, Testing Loss : 0.4005739471435547, Training Accuracy : 0.92602, Testing Accuracy : 0.9029\n",
      "Epoch : 160, Training Loss : 0.20115229583740235, Testing Loss : 0.37742337265014647, Training Accuracy : 0.92968, Testing Accuracy : 0.9088\n",
      "Epoch : 170, Training Loss : 0.19052874839782716, Testing Loss : 0.377939501953125, Training Accuracy : 0.93222, Testing Accuracy : 0.9068\n",
      "Epoch : 180, Training Loss : 0.18932303756713867, Testing Loss : 0.3778273422241211, Training Accuracy : 0.93332, Testing Accuracy : 0.9115\n",
      "Epoch : 190, Training Loss : 0.18357444732666015, Testing Loss : 0.3645595031738281, Training Accuracy : 0.93524, Testing Accuracy : 0.9114\n",
      "Epoch : 200, Training Loss : 0.18346001235961915, Testing Loss : 0.3647276344299316, Training Accuracy : 0.93614, Testing Accuracy : 0.9128\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation: df.addTrivialAugmentation()\n",
    "model = ResNet(BasicBlock, 32, 4, [4, 4, 4, 2],kernel=(3,1),skip_kernel=(1,0), num_classes=10, bias=True).to(device)\n",
    "main(model, data_augmentation=['trivial_aug'], epochs=200, optim='adadelta', batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yoQwhlNtXMfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing fetching CIFAR10 dataset using torchvision\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Total Trainable Parameters : 3576842\n",
      "Total Epochs : 200 | Optimizer : adadelta | Learning Rate : 0.1 | Batch Size : 512\n",
      "Data Augmentation : ['trivial_aug', 'horizontal_flip', 'random_crop']\n",
      "Epoch : 0, Training Loss : 1.9790977270507812, Testing Loss : 4.803305517578125, Training Accuracy : 0.26556, Testing Accuracy : 0.1689\n",
      "Epoch : 10, Training Loss : 1.0910157543945314, Testing Loss : 0.9117696990966797, Training Accuracy : 0.61382, Testing Accuracy : 0.6817\n",
      "Epoch : 20, Training Loss : 0.8085469342041015, Testing Loss : 0.6477414871215821, Training Accuracy : 0.71572, Testing Accuracy : 0.7758\n",
      "Epoch : 30, Training Loss : 0.6758965310668945, Testing Loss : 0.4842376159667969, Training Accuracy : 0.76178, Testing Accuracy : 0.8418\n",
      "Epoch : 40, Training Loss : 0.5987245880126953, Testing Loss : 0.44454466400146486, Training Accuracy : 0.7893, Testing Accuracy : 0.8555\n",
      "Epoch : 50, Training Loss : 0.5150775448608399, Testing Loss : 0.3942409858703613, Training Accuracy : 0.81946, Testing Accuracy : 0.8729\n",
      "Epoch : 60, Training Loss : 0.46730686126708987, Testing Loss : 0.38311985931396486, Training Accuracy : 0.83536, Testing Accuracy : 0.8785\n",
      "Epoch : 70, Training Loss : 0.41723058380126954, Testing Loss : 0.3092850463867188, Training Accuracy : 0.8531, Testing Accuracy : 0.9028\n",
      "Epoch : 80, Training Loss : 0.3951516665649414, Testing Loss : 0.3165670639038086, Training Accuracy : 0.8617, Testing Accuracy : 0.9047\n",
      "Epoch : 90, Training Loss : 0.372326823425293, Testing Loss : 0.29939010162353513, Training Accuracy : 0.86894, Testing Accuracy : 0.9066\n",
      "Epoch : 100, Training Loss : 0.33708052108764647, Testing Loss : 0.2743022300720215, Training Accuracy : 0.88224, Testing Accuracy : 0.9195\n",
      "Epoch : 110, Training Loss : 0.3247777325439453, Testing Loss : 0.2904986045837402, Training Accuracy : 0.88552, Testing Accuracy : 0.9177\n",
      "Epoch : 120, Training Loss : 0.30660850234985354, Testing Loss : 0.2769919731140137, Training Accuracy : 0.89134, Testing Accuracy : 0.9256\n",
      "Epoch : 130, Training Loss : 0.28900729339599607, Testing Loss : 0.25980481643676756, Training Accuracy : 0.89924, Testing Accuracy : 0.9281\n",
      "Epoch : 140, Training Loss : 0.2714821376037598, Testing Loss : 0.2689222877502441, Training Accuracy : 0.90538, Testing Accuracy : 0.9277\n",
      "Epoch : 150, Training Loss : 0.2629083657836914, Testing Loss : 0.2646102386474609, Training Accuracy : 0.90694, Testing Accuracy : 0.9275\n",
      "Epoch : 160, Training Loss : 0.2524164109802246, Testing Loss : 0.2513238540649414, Training Accuracy : 0.91218, Testing Accuracy : 0.9339\n",
      "Epoch : 170, Training Loss : 0.24383530456542968, Testing Loss : 0.25457830505371093, Training Accuracy : 0.91432, Testing Accuracy : 0.9348\n",
      "Epoch : 180, Training Loss : 0.23645921493530273, Testing Loss : 0.24998167419433595, Training Accuracy : 0.91676, Testing Accuracy : 0.938\n",
      "Epoch : 190, Training Loss : 0.22882271194458007, Testing Loss : 0.2493404754638672, Training Accuracy : 0.92072, Testing Accuracy : 0.9366\n",
      "Epoch : 200, Training Loss : 0.22937976226806642, Testing Loss : 0.24587747955322264, Training Accuracy : 0.91938, Testing Accuracy : 0.9374\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(BasicBlock, 32, 4, [4, 4, 4, 2],kernel=(3,1),skip_kernel=(1,0), num_classes=10, bias=True).to(device)\n",
    "main(model, data_augmentation=['trivial_aug', 'horizontal_flip', 'random_crop'], epochs=200, optim='adadelta', batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCkj0gYIXMcI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0g8f6ZdXMZo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtqNoBPwm0fd",
    "outputId": "fe45ed25-c733-439f-e203-16d9894b07c9"
   },
   "outputs": [],
   "source": [
    "# # Data Augmentation: df.addHorizontalFlipping(), df.addRandomCrop(size=32, padding=3), df.addHistogramEqualization(), df.addNormalizer()\n",
    "# model = ResNet(BasicBlock, 32, 4, [4, 4, 4, 2],kernel=(3,1),skip_kernel=(1,0), num_classes=10, bias=True).to(device)\n",
    "# main(model, epochs=100, optim='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
